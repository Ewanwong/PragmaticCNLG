{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from error_analysis_diagnosis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "from PragmaticGPT2 import PragmaticGPT2LMHeadModel\n",
    "\n",
    "model = 'gpt2-xl'\n",
    "model = PragmaticGPT2LMHeadModel(model, 0, 0, 2, 'sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5285, 0.4715]], device='cuda:0')\n",
      "tensor([[0.2446, 0.7554]], device='cuda:0')\n",
      "tensor([[0.4475, 0.5525]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sensitive to prompt\n",
    "TARGET_PREFIXES = [\"The following sentences contain [BLANK]:\",]\n",
    "DISTRACTOR_PREFIXES = [\"The following sentences don't contain [BLANK]:\",]\n",
    "\n",
    "attribute_name = 'identity_attack'\n",
    "target_prompt = fill_in_prefix(TARGET_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "distractor_prompt = fill_in_prefix(DISTRACTOR_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "input_text = [\" mind is the fact that in my major year of college I would have been an eight-year-\"]\n",
    "\n",
    "print(model.classify(input_text, target_prompt, distractor_prompt))\n",
    "\n",
    "TARGET_PREFIXES = [\"The following text contains [BLANK]:\",]\n",
    "DISTRACTOR_PREFIXES = [\"The following text doesn't contain [BLANK]:\",]\n",
    "\n",
    "attribute_name = 'identity_attack'\n",
    "target_prompt = fill_in_prefix(TARGET_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "distractor_prompt = fill_in_prefix(DISTRACTOR_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "input_text = [\" mind is the fact that in my major year of college I would have been an eight-year-\"]\n",
    "\n",
    "print(model.classify(input_text, target_prompt, distractor_prompt))\n",
    "\n",
    "TARGET_PREFIXES = [\"Write sentences that contain [BLANK]:\",]\n",
    "DISTRACTOR_PREFIXES = [\"Write sentences that don't contain [BLANK]:\",]\n",
    "\n",
    "attribute_name = 'identity_attack'\n",
    "target_prompt = fill_in_prefix(TARGET_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "distractor_prompt = fill_in_prefix(DISTRACTOR_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "input_text = [\" mind is the fact that in my major year of college I would have been an eight-year-\"]\n",
    "\n",
    "print(model.classify(input_text, target_prompt, distractor_prompt))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1974, 0.8026]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "TARGET_PREFIXES = [\"The following sentences contain a threat:\",]\n",
    "DISTRACTOR_PREFIXES = [\"The following sentences contain no threat:\",]\n",
    "\n",
    "attribute_name = 'identity_attack'\n",
    "target_prompt = fill_in_prefix(TARGET_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "distractor_prompt = fill_in_prefix(DISTRACTOR_PREFIXES, DESCRIPTIONS, attribute_name)\n",
    "input_text = [\" a person's behavior and influences what they think. They might assume a gun is going to kill them\"]\n",
    "\n",
    "print(model.classify(input_text, target_prompt, distractor_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = load_model_outputs('realtoxicityprompts-data/generations/prompted/prompted_gens_gpt2.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "batch_sizes = [1, 8, 4, 2]\n",
    "attributes = ['toxicity', 'severe_toxicity', 'sexually_explicit', 'threat', 'identity_attack', 'profanity']\n",
    "output_filename = 'jupyter_debug.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perplexity import *\n",
    "model_name = 'gpt2'\n",
    "alpha=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "model = PragmaticGPT2LMHeadModel(model_name, alpha, 0, len(TARGET_PREFIXES)+len(DISTRACTOR_PREFIXES), 'mean')\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33])\n",
      "logits shape: torch.Size([8, 51, 50257])\n",
      "regular_logits shape: torch.Size([1, 51, 50257])\n",
      "real_labels shape: torch.Size([1, 51])\n",
      "real_input_length: 33\n",
      "real_output_logits shape: torch.Size([8, 33, 50257])\n",
      "real_prior_distribution shape: torch.Size([1, 33, 7])\n",
      "other_prob_by_example shape: torch.Size([1, 33, 7, 50257])\n",
      "pragmatic_listener_probability_distribution shape: torch.Size([1, 33, 7, 50257])\n",
      "pragmatic_speaker_probability_distribution shape: torch.Size([1, 33, 50257])\n",
      "pragmatic_logits shape: torch.Size([1, 33, 50257])\n",
      "real_labels shape: torch.Size([1, 33])\n",
      "real_logits shape: torch.Size([1, 33, 50257])\n",
      "tensor(4.8857, device='cuda:0') tensor(11.7989, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sent = ['Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors']\n",
    "print(tokenizer(sent, return_tensors='pt')['input_ids'].shape)\n",
    "loss_regular, loss_debiased = model.compute_perplexity(sent, TARGET_PREFIXES, DISTRACTOR_PREFIXES)\n",
    "print(loss_regular, loss_debiased)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\"\n",
    "sent = ['I love it', 'Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors']\n",
    "inputs = tokenizer(sent, return_tensors='pt', padding=True, truncation=True) \n",
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "print(tokenizer.batch_decode(gpt_model.generate(**inputs, min_length=50, max_length=50, do_sample=False), skip_special_tokens=True))\n",
    "labels = input_ids.clone()\n",
    "labels.masked_fill_(attention_mask==0, -100)\n",
    "output = gpt_model(**inputs, labels=labels)\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = config.n_positions - 50\n",
    "\n",
    "stride = max_length\n",
    "\n",
    "lls_debiased, lls_regular = [], []\n",
    "ppl_debiased, ppl_regular = None, None\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "    trg_len = end_loc - i  # may be different from stride on last loop\n",
    "    \n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    input = tokenizer.batch_decode(input_ids)\n",
    "    print(trg_len)\n",
    "    loss_regular, loss_debiased = model.compute_perplexity(input, TARGET_PREFIXES, DISTRACTOR_PREFIXES)\n",
    "    \n",
    "    log_likelihood_debiased = loss_debiased * trg_len\n",
    "    log_likelihood_regular = loss_regular * trg_len\n",
    "\n",
    "    lls_debiased.append(log_likelihood_debiased)\n",
    "    lls_regular.append(log_likelihood_regular)\n",
    "\n",
    "    ppl_debiased = torch.exp(torch.stack(lls_debiased).sum() / end_loc)\n",
    "    ppl_regular = torch.exp(torch.stack(lls_regular).sum() / end_loc)\n",
    "    print(f'Perplexity after {i} tokens: {ppl_debiased} (debiased) vs {ppl_regular} (regular)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-debias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
